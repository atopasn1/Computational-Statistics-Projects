---
title: "Project3"
output: html_document
date: "2025-10-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
library(dplyr)
library(lubridate)
library(forcats)
library(ggplot2)
library(tidytext)

```

```{r}
#Loading in the Data

library("here")
rds_files <- c("b_lyrics.RDS", "ts_lyrics.RDS", "sales.RDS")
## Check whether we have all 3 files
if (any(!file.exists(here("data", rds_files)))) {
    ## If we don't, then download the data
    b_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv")
    ts_lyrics <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv")
    sales <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/sales.csv")

    ## Then save the data objects to RDS files
    saveRDS(b_lyrics, file = here("data", "b_lyrics.RDS"))
    saveRDS(ts_lyrics, file = here("data", "ts_lyrics.RDS"))
    saveRDS(sales, file = here("data", "sales.RDS"))
}


b_lyrics <- readRDS(here("data", "b_lyrics.RDS"))
ts_lyrics <- readRDS(here("data", "ts_lyrics.RDS"))
sales <- readRDS(here("data", "sales.RDS"))
```

Part 1:Explore Album Sales

Part 1A
```{r}
#Use lubridate to create a column called released that is a Date class. However, to be able to do this, you first need to use stringr to search for pattern that matches things like this “(US)[51]” in a string like this “September 1, 2006 (US)[51]” and removes them. (Note: to get full credit, you must create the regular expression).

#String Search/Regular Expression-we are looking at the released column and there are a few rows that have data after the release date

sales$released <- str_replace(string = sales$released, pattern = "\\(.+\\)\\[.+\\]$", replace = " ")
#still am confused why this needs to have the plus, is that something that needs to be specified each time
sales$released <- str_trim(sales$released)
#I am getting rid of the extra whitespace in order to make downstream functions easier to use
sales$released

#now we need to use lubridate
sales$released <- mdy(sales$released) #is this okay that it is not in the standard format?

class(sales$released)#shows now as date
```
```{r}
#Use forcats to create a factor called country (Note: you may need to collapse some factor levels).

unique(sales$country)

sales <- sales %>% 
  mutate(country = fct_collapse(country,
                                WW = c("World","WW")))

unique(sales$country)

class(sales$country)

```
```{r}
#Transform the sales into a unit that is album sales in millions of dollars.

sales <- sales %>% 
  mutate(sales = sales / 1000000)

sales <- sales %>% 
  rename("Sales (in Millions)" = "sales")

sales$`Sales (in Millions)`
```
```{r}
sales_final <- sales %>% 
  filter(country %in% c( "UK" , "US", "WW"))

sales_final
```

Part 1B
```{r}
#1A: Keep only album sales from the US.

sales_us <- sales_final %>% 
  filter(country == "US")

#Create a new column called years_since_release corresponding to the number of years since the release of each album from Beyoncé and Taylor Swift. This should be a whole number and you should round down to “14” if you get a non-whole number like “14.12” years. (Hint: you may find the interval() function from lubridate helpful here, but this not the only way to do this.)

sales_us$years_since_release <- year(today()) - year(sales_us$released)
#do i need to do this in a way where the year is always up to date

#Calculate the most recent, oldest, and the median years since albums were released for both Beyoncé and Taylor Swift.

#does this make sense as median year?
sales_us %>% 
  group_by(artist) %>% 
  summarize(Median_album_age = median(years_since_release), Oldest_Release = max(years_since_release), Most_Recent_Release = min(years_since_release))



```

Part 1C
```{r}
#Calculate the total album sales for each artist and for each country (only sales from the UK, US, and World).Note: assume that the World sales do not include the UK and US ones.

data_sum <- sales_final %>% 
  group_by(artist, country) %>% 
  summarize("Sales(Millions)" =sum(`Sales (in Millions)`))

data_sum

ggplot(data_sum, aes(fill=artist, y=`Sales(Millions)`, x=country)) + 
    geom_bar(position="fill", stat="identity")

#need to make this look nicer and also make sure this is the thing they want
```
Part 1D
```{r}
world_sales <- sales_final %>% 
  filter(country == "WW")

ggplot(data =world_sales,aes(x = fct_reorder(title, `Sales (in Millions)`), y = `Sales (in Millions)`, fill = artist))+ #needto use backtick because of the way I named my columns!
  geom_col()+
  coord_flip()#so why does this work? I need to look at this again
#again need to add all the necesarry stuff

```
Part 1E
```{r}
ggplot(data = sales_final, aes(x=released, y =`Sales (in Millions)`, color= artist))+
  geom_point()+
  facet_wrap("country")

#again need to add all the stuff
```
Part 2:Exploring Sentiment of Lyrics

Part 2A
```{r}
#Using ts_lyrics, create a new column called line with one line containing the character string for each line of Taylor Swift’s songs.

ts_lyrics <-
    ts_lyrics %>%
    unnest_tokens(
        output = line,
        input = Lyrics,
        token = "lines"
    )

#How many lines in Taylor Swift’s lyrics contain the word “hello”? For full credit, show all the rows in ts_lyrics that have “hello” in the line column and report how many rows there are in total.

str_subset(ts_lyrics$line, "hello") # all the lyrics with "hello"
sum(str_count(ts_lyrics$line, "hello")) #number of times it appears

#How many lines in Taylor Swift’s lyrics contain the word “goodbye”? For full credit, show all the rows in ts_lyrics that have “goodbye” in the line column and report how many rows there are in total.

str_subset(ts_lyrics$line, "goodbye") # all the lyrics with "goodbye"
sum(str_count(ts_lyrics$line, "goodbye")) #number of times it appears

```
Part 2B
```{r}
#Repeat the same analysis for b_lyrics as described in Part 2A.

b_lyrics <-
    b_lyrics %>%
    unnest_tokens(
        output = line,
        input = line,
        token = "lines"
    )

str_subset(b_lyrics$line, "hello") # all the lyrics with "hello"
sum(str_count(b_lyrics$line, "hello")) #number of times it appears

str_subset(b_lyrics$line, "goodbye") # all the lyrics with "goodbye"
sum(str_count(b_lyrics$line, "goodbye")) #number of times it appears

```
Part 2C

```{r}
#Tokenize each lyrical line by words.
b_lyrics <-
    b_lyrics %>%
    unnest_tokens(
        output = word,
        input = line,
        token = "words"
    )

#Remove the “stopwords”.

no_stop <- b_lyrics %>% #note to self, created a seperate data set so I would not mess up the original one
  filter(!word %in% stop_words$word)

#Calculate the total number for each word in the lyrics.

b_sum <- no_stop %>%
  group_by(word) %>% 
  summarise(n=n()) 

b_sum %>% 
  slice_sample(n=10)#done to make things easier to view on the PDF

#Using the “bing” sentiment lexicon, add a column to the summarized data frame adding the “bing” sentiment lexicon.

b_sum <- b_sum %>%
    inner_join(get_sentiments("bing"), by ="word")

b_sum %>% 
  slice_sample(n=10)

#Sort the rows from most frequent to least frequent words

b_sum <- b_sum %>% 
  arrange(-n)

b_sum %>% 
  head()

#Only keep the top 25 most frequent words.

b25 <- b_sum[1:25,]

#Auto print the wrangled tibble data frame.
print(b25)

#Use ggplot2 to create a bar plot with the top words on the y-axis and the frequency of each word on the x-axis. Color each bar by the sentiment of each word from the “bing” sentiment lexicon. Bars should be ordered from most frequent on the top to least frequent on the bottom of the plot.

ggplot(data=b25, aes(x = fct_reorder(word, n), y = n, fill=sentiment))+
  geom_col()+
  coord_flip() #need to add all of the stuff to get full credit

#Create a word cloud of the top 25 most frequent words.
library(wordcloud)

b25 %>%
    with(wordcloud(word, n, max.words = 25))
```
Part 2D
```{r}
#Tokenize each lyrical line by words.
ts_lyrics <-
    ts_lyrics %>%
    unnest_tokens(
        output = word,
        input = line,
        token = "words"
    )

#Remove the “stopwords”.

no_stop_ts <- ts_lyrics %>% #note to self, created a seperate data set so I would not mess up the original one
  filter(!word %in% stop_words$word)

#Calculate the total number for each word in the lyrics.

ts_sum <- no_stop_ts %>%
  group_by(word) %>% 
  summarise(n=n()) 

ts_sum %>% 
  slice_sample(n=10)#done to make things easier to view on the PDF

#Using the “bing” sentiment lexicon, add a column to the summarized data frame adding the “bing” sentiment lexicon.

ts_sum <- ts_sum %>%
    inner_join(get_sentiments("bing"), by ="word")

ts_sum %>% 
  slice_sample(n=10)

#Sort the rows from most frequent to least frequent words

ts_sum <- ts_sum %>% 
  arrange(-n)

ts_sum %>% 
  head()

#Only keep the top 25 most frequent words.

ts25 <- ts_sum[1:25,]

#Auto print the wrangled tibble data frame.
print(ts25)

#Use ggplot2 to create a bar plot with the top words on the y-axis and the frequency of each word on the x-axis. Color each bar by the sentiment of each word from the “bing” sentiment lexicon. Bars should be ordered from most frequent on the top to least frequent on the bottom of the plot.

ggplot(data=ts25, aes(x = fct_reorder(word, n), y = n, fill=sentiment))+
  geom_col()+
  coord_flip() #need to add all of the stuff to get full credit

#Create a word cloud of the top 25 most frequent words.

ts25 %>%
    with(wordcloud(word, n, max.words = 25))
```
Part 2E

```{r}
#Tokenize each lyrical line by words.
#This was already done in part 2D but I am putting the code in here as a comment as a reminder

#ts_lyrics <-
    #ts_lyrics %>%
    #unnest_tokens(
        #output = word,
        #input = line,
        #token = "words"
    #)

#Remove the “stopwords”.

no_stop_ts <- ts_lyrics %>% 
  filter(!ts_lyrics$word %in% stop_words$word)

#Calculate the total number for each word in the lyrics for each Album.
ts_sum2 <- no_stop_ts %>%
  group_by(Album,word) %>% 
  summarise(n=n()) 

ts_sum2 %>% 
  head()

#Using the “afinn” sentiment lexicon, add a column to the summarized data frame adding the “afinn” sentiment lexicon.
library(textdata)

ts_sum2 <- ts_sum2 %>%
    inner_join(get_sentiments("afinn"), by ="word")

ts_sum2 %>% 
  head()

#Calculate the average sentiment score for each Album.
sumdat <- ts_sum2 %>% 
  group_by(Album) %>% 
  summarize(mean = mean(value))

#Auto print the wrangled tibble data frame.
print(sumdat)

#Join the wrangled data frame from Part 1A (album sales in millions) filtered down to US sales with the wrangled data frame from #6 above (average sentiment score for each album).
#this will be a combination of sumdat and sales_us
#need to rename sales_us title to album

sales_us <- sales_us %>% 
  rename(Album = title)

taylor_all <- inner_join(sumdat, sales_us, by = "Album")

print(taylor_all)

#Using ggplot2, create a scatter plot of the average sentiment score for each album (y-axis) and the album release data along the x-axis. Make the size of each point the album sales in millions.

#Add a horizontal line at y-intercept=0.
ggplot(data = taylor_all, aes(x = released, y = mean, size = `Sales (in Millions)`))+
  geom_point()+
  geom_hline(yintercept = 0,color = "red", linetype = "dashed")#need to add all the stuff here

#Write 2-3 sentences interpreting the plot answering the question “How has the sentiment of Taylor Swift’s albums have changed over time?”. Add a title, subtitle, and useful axis labels.
```
```{r}
sessioninfo::session_info()
```

